{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils  import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "\n",
    "#plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ...\n",
       "0    ...\n",
       "1    ...\n",
       "10   ...\n",
       "100  ...\n",
       "1000 ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview of text data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# data clean => remove emails and newline charaters\n",
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from',\n",
      "  'wheres',\n",
      "  'my',\n",
      "  'thing',\n",
      "  'subject',\n",
      "  'what',\n",
      "  'car',\n",
      "  'is',\n",
      "  'this',\n",
      "  'nntp',\n",
      "  'posting',\n",
      "  'host',\n",
      "  'rac',\n",
      "  'wam',\n",
      "  'umd',\n",
      "  'edu',\n",
      "  'organization',\n",
      "  'university',\n",
      "  'of',\n",
      "  'maryland',\n",
      "  'college',\n",
      "  'park',\n",
      "  'lines',\n",
      "  'was',\n",
      "  'wondering',\n",
      "  'if',\n",
      "  'anyone',\n",
      "  'out',\n",
      "  'there',\n",
      "  'could',\n",
      "  'enlighten',\n",
      "  'me',\n",
      "  'on',\n",
      "  'this',\n",
      "  'car',\n",
      "  'saw',\n",
      "  'the',\n",
      "  'other',\n",
      "  'day',\n",
      "  'it',\n",
      "  'was',\n",
      "  'door',\n",
      "  'sports',\n",
      "  'car',\n",
      "  'looked',\n",
      "  'to',\n",
      "  'be',\n",
      "  'from',\n",
      "  'the',\n",
      "  'late',\n",
      "  'early',\n",
      "  'it',\n",
      "  'was',\n",
      "  'called',\n",
      "  'bricklin',\n",
      "  'the',\n",
      "  'doors',\n",
      "  'were',\n",
      "  'really',\n",
      "  'small',\n",
      "  'in',\n",
      "  'addition',\n",
      "  'the',\n",
      "  'front',\n",
      "  'bumper',\n",
      "  'was',\n",
      "  'separate',\n",
      "  'from',\n",
      "  'the',\n",
      "  'rest',\n",
      "  'of',\n",
      "  'the',\n",
      "  'body',\n",
      "  'this',\n",
      "  'is',\n",
      "  'all',\n",
      "  'know',\n",
      "  'if',\n",
      "  'anyone',\n",
      "  'can',\n",
      "  'tellme',\n",
      "  'model',\n",
      "  'name',\n",
      "  'engine',\n",
      "  'specs',\n",
      "  'years',\n",
      "  'of',\n",
      "  'production',\n",
      "  'where',\n",
      "  'this',\n",
      "  'car',\n",
      "  'is',\n",
      "  'made',\n",
      "  'history',\n",
      "  'or',\n",
      "  'whatever',\n",
      "  'info',\n",
      "  'you',\n",
      "  'have',\n",
      "  'on',\n",
      "  'this',\n",
      "  'funky',\n",
      "  'looking',\n",
      "  'car',\n",
      "  'please',\n",
      "  'mail',\n",
      "  'thanks',\n",
      "  'il',\n",
      "  'brought',\n",
      "  'to',\n",
      "  'you',\n",
      "  'by',\n",
      "  'your',\n",
      "  'neighborhood',\n",
      "  'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize words and clean-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence),deacc=True))\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "pprint(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigheiniu/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bigram_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e57a9499c074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# see result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bigram_mod' is not defined"
     ]
    }
   ],
   "source": [
    "#build bigram data and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold= 100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words],threshold=100)\n",
    "\n",
    "bigram_mode = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mode = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# see result\n",
    "print(bigram_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, combine bigrams and trigrams and use lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mode[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mode[bigram_mode[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postages=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"]):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postages])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigheiniu/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/bigheiniu/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "\n",
    "# do lemmatization, keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and corpus needed for topic modeling\n",
    "# create dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "#create corpus\n",
    "texts = data_lemmatized\n",
    "#term document frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 5), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mallet model to do topic extraction\n",
    "mallel_path = './mallet-2.0.6/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallel_path,corpus=corpus,num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14,\n",
      "  [('gun', 0.020417240587695132),\n",
      "   ('state', 0.01493629476584022),\n",
      "   ('law', 0.013874540863177227),\n",
      "   ('people', 0.012009297520661157),\n",
      "   ('article', 0.007848370064279155),\n",
      "   ('make', 0.007475321395775942),\n",
      "   ('write', 0.007417929292929293),\n",
      "   ('fire', 0.00726010101010101),\n",
      "   ('crime', 0.0071883608815426995),\n",
      "   ('weapon', 0.006815312213039486)]),\n",
      " (1,\n",
      "  [('ax', 0.8532514838840757),\n",
      "   ('max', 0.06244648584323799),\n",
      "   ('tm', 0.002278433250613145),\n",
      "   ('qax', 0.0020317239177442058),\n",
      "   ('giz', 0.0010158619588721029),\n",
      "   ('mf', 0.00100134964517393),\n",
      "   ('_', 0.000870738821890374),\n",
      "   ('ml', 0.0007836649397013366),\n",
      "   ('gq', 0.0007691526260031636),\n",
      "   ('wm_wm', 0.0007691526260031636)]),\n",
      " (2,\n",
      "  [('window', 0.01775762572135202),\n",
      "   ('line', 0.01749381698268755),\n",
      "   ('file', 0.016224237427864798),\n",
      "   ('program', 0.012844187963726298),\n",
      "   ('set', 0.011838417147568014),\n",
      "   ('problem', 0.010255564715581204),\n",
      "   ('entry', 0.00852431986809563),\n",
      "   ('write', 0.008375927452596868),\n",
      "   ('read', 0.007518549051937345),\n",
      "   ('application', 0.007353668590272052)]),\n",
      " (0,\n",
      "  [('space', 0.02462498774469754),\n",
      "   ('launch', 0.007483904702768065),\n",
      "   ('system', 0.007385862283081146),\n",
      "   ('earth', 0.006650544135429262),\n",
      "   ('year', 0.006470799699336579),\n",
      "   ('nasa', 0.00630739566652505),\n",
      "   ('center', 0.005768162358247002),\n",
      "   ('satellite', 0.005751821954965848),\n",
      "   ('project', 0.005686460341841237),\n",
      "   ('research', 0.005588417922154319)]),\n",
      " (19,\n",
      "  [('time', 0.016385927226049826),\n",
      "   ('people', 0.016176355015324967),\n",
      "   ('start', 0.014198517276609122),\n",
      "   ('day', 0.011683650747910827),\n",
      "   ('back', 0.011565766379378095),\n",
      "   ('happen', 0.01140858722133445),\n",
      "   ('leave', 0.010845361905011395),\n",
      "   ('hear', 0.010269038325518036),\n",
      "   ('thing', 0.00999397479894166),\n",
      "   ('put', 0.00818641448143976)]),\n",
      " (16,\n",
      "  [('book', 0.01693088233180505),\n",
      "   ('point', 0.012863446253790765),\n",
      "   ('number', 0.009730226942811561),\n",
      "   ('science', 0.009629618983284707),\n",
      "   ('find', 0.009184069448237206),\n",
      "   ('reference', 0.00892536326659672),\n",
      "   ('information', 0.007646204924040991),\n",
      "   ('include', 0.007488106701927361),\n",
      "   ('university', 0.0074018713080472),\n",
      "   ('give', 0.006870086379119537)]),\n",
      " (8,\n",
      "  [('game', 0.0242732628913668),\n",
      "   ('team', 0.017614549349953743),\n",
      "   ('year', 0.013840872571456395),\n",
      "   ('play', 0.013792179967862882),\n",
      "   ('good', 0.01322004187563909),\n",
      "   ('player', 0.01180795637142718),\n",
      "   ('win', 0.011174952524711497),\n",
      "   ('season', 0.00757169985879145),\n",
      "   ('hockey', 0.006792618201295223),\n",
      "   ('fan', 0.006220480109071432)]),\n",
      " (15,\n",
      "  [('_', 0.05344902118199222),\n",
      "   ('organization', 0.03103192737602668),\n",
      "   ('line', 0.02689433705922312),\n",
      "   ('ca', 0.015438769838819243),\n",
      "   ('air', 0.006237263014882974),\n",
      "   ('cx', 0.005835854999073674),\n",
      "   ('newsreader_tin', 0.005804977459396035),\n",
      "   ('ad', 0.0037361823009942566),\n",
      "   ('md', 0.0037053047613166182),\n",
      "   ('ed', 0.0037053047613166182)]),\n",
      " (4,\n",
      "  [('god', 0.017276770441327402),\n",
      "   ('christian', 0.016434643016921498),\n",
      "   ('man', 0.011921366351746098),\n",
      "   ('bible', 0.008447590726071739),\n",
      "   ('people', 0.008408116003052711),\n",
      "   ('church', 0.008368641280033685),\n",
      "   ('love', 0.007658096265691202),\n",
      "   ('word', 0.007131766625437511),\n",
      "   ('law', 0.006210689754993552),\n",
      "   ('world', 0.005736993078765231)]),\n",
      " (10,\n",
      "  [('write', 0.014331379233064791),\n",
      "   ('people', 0.014178039372957927),\n",
      "   ('make', 0.013057478856792365),\n",
      "   ('question', 0.01164203399426745),\n",
      "   ('reason', 0.010533268851956263),\n",
      "   ('exist', 0.01047429198268439),\n",
      "   ('thing', 0.010403519739558146),\n",
      "   ('claim', 0.009837341794548178),\n",
      "   ('true', 0.008516259922858255),\n",
      "   ('wrong', 0.008374715436605763)])]\n"
     ]
    }
   ],
   "source": [
    "pprint(ldamallet.show_topics(formatted=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step =3):\n",
    "    coherence_values=[]\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path=mallel_path,texts=texts,num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model,texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dominant topic in each sentence\n",
    "def format_topics_sentences(ldamodel=ldamallet,corpus = corpus, texts=data):\n",
    "    #init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    # get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x:(x[1]), reverse=True)\n",
    "        #get dominant topic, prec contribution and keywords for each document\n",
    "        for j,(topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0: #dorminant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \",\".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num),round(protopic,4),topic_keywords],ignore_index=True))\n",
    "            else:\n",
    "                break\n",
    "        sent_topics_df.columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df], axis=1)\n",
    "        return(sent_topics_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = ldamallet[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(th[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
